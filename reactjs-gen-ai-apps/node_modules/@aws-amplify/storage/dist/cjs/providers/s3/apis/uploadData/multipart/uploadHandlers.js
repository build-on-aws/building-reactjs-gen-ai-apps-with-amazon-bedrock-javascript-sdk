'use strict';

// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
// SPDX-License-Identifier: Apache-2.0
Object.defineProperty(exports, "__esModule", { value: true });
exports.getMultipartUploadHandlers = void 0;
const core_1 = require("@aws-amplify/core");
const utils_1 = require("@aws-amplify/core/internals/utils");
const getDataChunker_1 = require("./getDataChunker");
const utils_2 = require("../../../utils");
const constants_1 = require("../../../utils/constants");
const initialUpload_1 = require("./initialUpload");
const progressTracker_1 = require("./progressTracker");
const uploadCache_1 = require("./uploadCache");
const uploadPartExecutor_1 = require("./uploadPartExecutor");
const StorageError_1 = require("../../../../../errors/StorageError");
const CanceledError_1 = require("../../../../../errors/CanceledError");
const client_1 = require("../../../utils/client");
const userAgent_1 = require("../../../utils/userAgent");
const utils_3 = require("../../../../../utils");
/**
 * Create closure hiding the multipart upload implementation details and expose the upload job and control functions(
 * onPause, onResume, onCancel).
 *
 * @internal
 */
const getMultipartUploadHandlers = ({ options: uploadDataOptions, key, data }, size) => {
    let resolveCallback;
    let rejectCallback;
    let inProgressUpload;
    let s3Config;
    let abortController;
    let bucket;
    let keyPrefix;
    let uploadCacheKey;
    // Special flag that differentiates HTTP requests abort error caused by pause() from ones caused by cancel().
    // The former one should NOT cause the upload job to throw, but cancels any pending HTTP requests.
    // This should be replaced by a special abort reason. However,the support of this API is lagged behind.
    let isAbortSignalFromPause = false;
    const startUpload = async () => {
        const resolvedS3Options = await (0, utils_2.resolveS3ConfigAndInput)(core_1.Amplify, uploadDataOptions);
        s3Config = resolvedS3Options.s3Config;
        bucket = resolvedS3Options.bucket;
        keyPrefix = resolvedS3Options.keyPrefix;
        abortController = new AbortController();
        isAbortSignalFromPause = false;
        const { contentDisposition, contentEncoding, contentType = 'application/octet-stream', metadata, accessLevel, onProgress, } = uploadDataOptions ?? {};
        if (!inProgressUpload) {
            const { uploadId, cachedParts } = await (0, initialUpload_1.loadOrCreateMultipartUpload)({
                s3Config,
                accessLevel: resolveAccessLevel(accessLevel),
                bucket,
                keyPrefix,
                key,
                contentType,
                contentDisposition,
                contentEncoding,
                metadata,
                data,
                size,
                abortSignal: abortController.signal,
            });
            inProgressUpload = {
                uploadId,
                completedParts: cachedParts,
            };
        }
        const finalKey = keyPrefix + key;
        uploadCacheKey = size
            ? (0, uploadCache_1.getUploadsCacheKey)({
                file: data instanceof File ? data : undefined,
                accessLevel: resolveAccessLevel(uploadDataOptions?.accessLevel),
                contentType: uploadDataOptions?.contentType,
                bucket: bucket,
                size,
                key,
            })
            : undefined;
        const dataChunker = (0, getDataChunker_1.getDataChunker)(data, size);
        const completedPartNumberSet = new Set(inProgressUpload.completedParts.map(({ PartNumber }) => PartNumber));
        const onPartUploadCompletion = (partNumber, eTag) => {
            inProgressUpload?.completedParts.push({
                PartNumber: partNumber,
                ETag: eTag,
            });
        };
        const concurrentUploadsProgressTracker = (0, progressTracker_1.getConcurrentUploadsProgressTracker)({
            size,
            onProgress,
        });
        const concurrentUploadPartExecutors = [];
        for (let index = 0; index < constants_1.DEFAULT_QUEUE_SIZE; index++) {
            concurrentUploadPartExecutors.push((0, uploadPartExecutor_1.uploadPartExecutor)({
                dataChunkerGenerator: dataChunker,
                completedPartNumberSet,
                s3Config,
                abortSignal: abortController.signal,
                bucket,
                finalKey,
                uploadId: inProgressUpload.uploadId,
                onPartUploadCompletion,
                onProgress: concurrentUploadsProgressTracker.getOnProgressListener(),
                isObjectLockEnabled: resolvedS3Options.isObjectLockEnabled,
            }));
        }
        await Promise.all(concurrentUploadPartExecutors);
        const { ETag: eTag } = await (0, client_1.completeMultipartUpload)({
            ...s3Config,
            abortSignal: abortController.signal,
            userAgentValue: (0, userAgent_1.getStorageUserAgentValue)(utils_1.StorageAction.UploadData),
        }, {
            Bucket: bucket,
            Key: finalKey,
            UploadId: inProgressUpload.uploadId,
            MultipartUpload: {
                Parts: inProgressUpload.completedParts.sort((partA, partB) => partA.PartNumber - partB.PartNumber),
            },
        });
        if (size) {
            const { ContentLength: uploadedObjectSize } = await (0, client_1.headObject)(s3Config, {
                Bucket: bucket,
                Key: finalKey,
            });
            if (uploadedObjectSize && uploadedObjectSize !== size) {
                throw new StorageError_1.StorageError({
                    name: 'Error',
                    message: `Upload failed. Expected object size ${size}, but got ${uploadedObjectSize}.`,
                });
            }
        }
        if (uploadCacheKey) {
            await (0, uploadCache_1.removeCachedUpload)(uploadCacheKey);
        }
        return {
            key,
            eTag,
            contentType,
            metadata,
        };
    };
    const startUploadWithResumability = () => startUpload()
        .then(resolveCallback)
        .catch(error => {
        const abortSignal = abortController?.signal;
        if (abortSignal?.aborted && isAbortSignalFromPause) {
            utils_3.logger.debug('upload paused.');
        }
        else {
            // Uncaught errors should be exposed to the users.
            rejectCallback(error);
        }
    });
    const multipartUploadJob = () => new Promise((resolve, reject) => {
        resolveCallback = resolve;
        rejectCallback = reject;
        startUploadWithResumability();
    });
    const onPause = () => {
        isAbortSignalFromPause = true;
        abortController?.abort();
    };
    const onResume = () => {
        startUploadWithResumability();
    };
    const onCancel = (message) => {
        // 1. abort in-flight API requests
        abortController?.abort(message);
        const cancelUpload = async () => {
            // 2. clear upload cache.
            if (uploadCacheKey) {
                await (0, uploadCache_1.removeCachedUpload)(uploadCacheKey);
            }
            // 3. clear multipart upload on server side.
            await (0, client_1.abortMultipartUpload)(s3Config, {
                Bucket: bucket,
                Key: keyPrefix + key,
                UploadId: inProgressUpload?.uploadId,
            });
        };
        cancelUpload().catch(e => {
            utils_3.logger.debug('error when cancelling upload task.', e);
        });
        rejectCallback(
        // Internal error that should not be exposed to the users. They should use isCancelError() to check if
        // the error is caused by cancel().
        new CanceledError_1.CanceledError(message ? { message } : undefined));
    };
    return {
        multipartUploadJob,
        onPause,
        onResume,
        onCancel,
    };
};
exports.getMultipartUploadHandlers = getMultipartUploadHandlers;
const resolveAccessLevel = (accessLevel) => accessLevel ??
    core_1.Amplify.libraryOptions.Storage?.S3?.defaultAccessLevel ??
    constants_1.DEFAULT_ACCESS_LEVEL;
//# sourceMappingURL=uploadHandlers.js.map
